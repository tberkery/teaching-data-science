---
title: "Data Conditioning in R: An Introduction to the Tidyverse"
author: "Tad Berkery" # Replace my name with your name
date: "2023-10-20" # Change this to today's date
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Tutorial developed by Tad Berkery. Designed to be completed in RStudio.

# R Background

**Libraries** in R are a collection of code that other people have written that you get to use. One of the beautiful things about code is that it scales. If I've written the line-by-line instructions on how to build a Linear Regression model, you don't have to and can just run the instructions I wrote on your machine. Libraries tend to specialize in specific tasks. With a quick Google search and some knowledge of what you are looking for, you can find libraries that will deliver functionality for your projects. 

To use a library, you use the syntax `library([Name of Library])`. To make things easy since this is an introductory tutorial, I've specified all of the libraries you will need for this project. Click the green triangular button to run the following code cell.

### Loading Libraries
```{r warning = FALSE, include = TRUE, message = FALSE}
library(tidyverse) # Great for data manipulation
library(reactable) # This library lets us display our data in a pretty and convenient format
library(ggplot2) # This library assists with data visualization.
```

What happened? Did you get lots of errors? That's because these libraries haven't been *installed* locally onto your computer yet. Fortunately, installing them is easy and a one-time process! If running the previous code cell unsuccessfully popped up a message that contains a button labeled "Install" to install the libraries, click "Install". Otherwise, copy and paste the following lines into the "Console", which is located right under this coding window. Once you have pasted them in the Console, hit enter on your keyboard.

install.packages("tidyverse@2.0.0")
install.packages("reactable@0.4.1")
install.packages("ggplot2@3.4.3")

Note that the general syntax for installing packages is install.packages("[package name]"). I'm adding the @ character and the following [#].[#].[#] to specify specific versions of each package to ensure you and I get the same output when we each separately run this tutorial, but, in general, you don't need to do this.

### Variables and Functions

Now that libraries have been installed and loaded, let's explore the bread and butter of coding in R (or more or less any coding language). A **variable** is a named object that stores information. Variables can take many forms.

```{r}
string = "I love watching baseball." # This is a variable consisting of words
integer1 = 3 # This is a variable storing the number 3
integer2 = 4 # This is a variable storing the number 4
boolean = TRUE # This is a boolean variable, indicating it's possible values are true and false
```

A **function** is a collection of lines of code that accomplish a specific, focused task. Functions take what are typically called **arguments** or **parameters**, which represent information that is passed into the function from outside sources. Functions can use the syntax `return([Variable of Interest])`, in which case they will populate a variable that they are assigned to with the value of `[Variable of Interest]`.

```{r}
# Here, we define functions

add = function(value1, value2) {
  answer = value1 + value2
  return(answer)
}

subtract = function(value1, value2) {
  answer = value1 - value2
  return(answer)
}

multiply = function(value1, value2) {
  answer = value1 * value2
  return(answer)
}

divide = function(value1, value2) {
  answer = value1 / value2
  return(answer)
}

square = function(value) {
  answer = value ^ 2
  return(answer)
}

```
The above functions represent the key tasks that need to be accomplished by a basic calculator. Note that these functions are presented solely for their simplicity: there are existing operators in R (namely `+`, `-`, `*`, and `/`) which already deliver this functionality in a much more robust manner. However, there are two key things to notice through this example. Notice the syntax with which we can call these functions using our variables defined earlier. Also notice that when we put a variable's name on its own line it prints when we run the code cell.
```{r}
# Here, we call our previously defined functions

sum = add(integer1, integer2)
sum
difference = subtract(integer1, integer2)
difference
product = multiply(integer1, integer2)
product
quotient = divide(integer1, integer2)
quotient
```
Note that in the *Environment* tab on the top right you can see the variables and functions we have defined listed. You can specifically also see the values stored in each of our variables.

Finally, to connect the ideas, note that when you load a library using `library([Name of Library])`, you, pivotally, gain the ability to call any function defined in that library. This is the fundamental reason we use libraries!

### Dataframes
One particular type of variable that we will work with a lot is called a **dataframe**. Dataframes can be thought of just like any table. Envision a basic spreadsheet in Microsoft Excel or Google Sheets, where data is in rows and columns. You will see that practically all of our data of interest in projects will be in the format of dataframes.

# The Tidyverse
At this point, we have at least motivated the ideas and fundamentals surrounding the bedrock of data analysis in R. The [**tidyverse**](https://www.tidyverse.org/packages/) is an extremely powerful library (or perhaps more specifically a collection of libraries) that provides extremely powerful tools for interacting with dataframes.

## Acquire Data
We will work through several examples showcasing how to use the tidyverse and aim to make this both interesting and actionable by working with real baseball data.

A **CSV**, which stands for "comma separated variables," is a type of file where content in different columns is separated by commas and content in different rows is separated by new-line characters (that is, the character corresponding to the "enter" or "return" key on your keyboard). CSVs are ubiquitous in data science and sports analytics. They are a very efficient and accessible way to encode tables. Think of a CSV as data in an Excel or Google Sheets spreadsheet. In fact, you can easily save an Excel or Google Sheets spreadsheet as a CSV and easily load the data into R.

Now, we will look to acquire data for some examples. There are three big websites for baseball data and analytics that you will likely rely on the most: [**Fangraphs**](https://www.fangraphs.com/) (which is by far the farthest along analytically), [**Baseball Reference**](https://www.baseball-reference.com/) (typically the best for getting raw data), and [**Baseball Savant**](https://baseballsavant.mlb.com/). If you are looking for more ways to learn about baseball analytics, [Fangraphs](https://www.fangraphs.com/) articles are a great place to start!

For this notebook, we will get our data from *Baseball Reference*, specifically utilizing the 2023 batting leaderboard. Navigate to this link in your browser: [2023 batting leaderboard data](https://www.baseball-reference.com/leagues/majors/2023-standard-batting.shtml). The screen you get should look like the following:

![Getting Data from *Baseball Reference*](./images/baseball_reference_screen_1.png)
Scroll down to where it says *Player Standard Batting*. When you find this, your screen should look like the following:
![Getting Data from *Baseball Reference*](./images/baseball_reference_screen_2.png)
As shown below, click "Share & Export" to the right of the "Player Standard Batting" header.
![Getting Data from *Baseball Reference*](./images/baseball_reference_screen_3.png)
Click "Get table as CSV (for Excel)". A few notes on this:

* It is better to use "Get table as CSV (for Excel)" than "Get as Excel Workbook" because the Excel Workbook direct option ahs a limit on how much rows of data can be extracted that is often less than the overall size of the dataset.

* A **CSV**, which stands for "comma separated variables," is a type of file where content in different columns is separated by commas and content in different rows is separated by new-line characters (that is, the character corresponding to the "enter" or "return" key on your keyboard). CSVs are ubiquitous in data science and sports analytics. They are a very efficient and accessible way to encode tables. Think of a CSV as data in an Excel or Google Sheets spreadsheet. In fact, you can easily save an Excel or Google Sheets spreadsheet as a CSV and easily load the data into R.

After clicking "Get table as CSV (for Excel)," your screen should look like the following (note that I'm only showing the first 10 batters, but you should have considerably more):

![Getting Data from *Baseball Reference*](./images/baseball_reference_screen_4.png)

Open a program of your choice that can work with text files. I recommend `Notepad` (especially if you are on a PC), but there are lots of basic programs that can perform this functionality. Paste this data. Save this file as a `.csv` file (you will likely need to change the file format to `.csv` when saving, as it is unlikely to be the default for this program). Make sure to name the file `br_batting_leaderboard_2023.csv` and to **place it in the same folder as where you have saved this notebook**.

Now, we can read this CSV file into a dataframe in R using the `read_csv` function.

```{r, warning = FALSE, message = FALSE}
batting_leaderboard_23 = read_csv("br_batting_leaderboard_2023.csv")
```
Let's take a quick look at our data:
```{r}
reactable(batting_leaderboard_23, searcable = TRUE, filterable = TRUE)
```
Note that `reactable` is a function in the `reactable` library I loaded that simply lets us view dataframes more nicely and interactively. It would have been perfectly okay to instead just include `batting_leaderboard_23` to display the dataframe without using `reactable`, but we use it here to demonstrate its usefulness in displaying dataframes. 

We need to clean up this data a little. Much of this you will do as we explore different functions in R below, but, to start, I will take care of some of the more complex stuff for you using the following functions below I defined. No need to understand the code with*in* these functions. Just make sure you undertand the idea that I can take a dataframe, design and call functions on it, and have transformed it into an updated dataframe that better suits my needs for analysis.

```{r}
remove_lg_avg_summary = function(df) {
  df = df[-nrow(df), ]
  df = filter(df, Tm != 'TOT')
  return(df)
}
remove_special_characters = function(df, special_characters) {
  df[] <- lapply(df, function(x) gsub(special_characters, "", x))
  return(df)
}

identify_numeric_columns = function(df) {
  is_numeric_column <- function(x) {
    all(grepl("^\\d+\\.?\\d*$", x))
  }
  for (col_name in names(df)) {
    if (is_numeric_column(df[[col_name]])) {
      df[[col_name]] <- as.numeric(df[[col_name]])
    }
  }
  return(df)
}

batting_leaderboard_23 = remove_lg_avg_summary(batting_leaderboard_23)

batting_leaderboard_23 = remove_special_characters(batting_leaderboard_23, "[*#]")

batting_leaderboard_23 = identify_numeric_columns(batting_leaderboard_23)

```

Here, note that I define three functions myself: `remove_lg_avg_summary`, `remove_special_characters`, and `identify_numeric_columns.` These functions are somewhat complicated and firmly beyond the scope of this tutorial. At a high level, `remove_lg_avg_summary` removes the last row of the initial *Baseball Reference* dataframe (which is desirable because this is league-average row inserted by baseball reference that does not actually correspond to a player season). `remove_special_characters` removes asterisks, number signs, and other special characters appended to player names indicating footnotes and additional references on the *Baseball Reference* website, which is both not of interest and problematic for us since we will want to ensure that, e.g., Shohei Ohtani is always listed as `Shohei Ohtani` and never `Shohei Ohtani*` or `Shohei Ohtani#` for lookup purposes. Finally, `identify_numeric_columns` tells R that columns that look like they contain numbers should be interpreted as containing information about a numeric variable. This is trivial to a human but important for how R handles various operations when it can't detect this automatically. No need to understand the code inside any of these functions... it's a little more complicated than we want to focus on right now. Just understand the idea that I can take a dataframe and pass it as an argument to a function to manipulate it in a way that better suits my needs.

Let's take one more look at our data: much cleaner now!
```{r}
reactable(batting_leaderboard_23, searchable = TRUE, filterable = TRUE)
```

## The Piping Operator
In R, the sequence of characters `%>%` is special: it is called the **piping operator**. The piping operator will likely seem weird and confusing at first but in the end will be second-nature and uber-convenient. The piping operator is defined in the form of [object] %>% [name of function that takes at least one object]. The key of the piping operator is that it takes the object before the pipe and passes it as the first argument to the function immediately following the piping operator. 

Here's a simple example, using the calculator-style functions we defined above.

```{r}
var1 = 3
var2 = 12
sum = add(var1, var2) # traditional method presented above
sum # display result
new_sum = var1 %>% add(var2) # same operation but using piping operator
new_sum # display result (it will be the same)
```

In this simple example, note that you have performed the same operation in both cases (just with different syntax). In both cases, the add operator is receiving two arguments (`var1` and `var2`, in that order).

Now let's try some slightly more complex examples.
Suppose I wanted to use the above functions to compute `3 + 6 - 5 + 4`. We know that this equals `8`. Let's use our functions to arrive at this answer and do it both using the traditional format and the piping operator.

```{r}
# Traditional Method
sum_terms_1_and_2 = add(3, 6)
sum_terms_2_and_3 = subtract(sum_terms_1_and_2, 5)
sum = add(sum_terms_2_and_3, 4)
sum
```
Now consider doing this with the piping operator:
```{r}
# Piping Method
sum = add(3, 6) %>%
  subtract(5) %>%
  add(4)
sum
```
Again, we get the same result and have performed the same operations. However, note in this more complex example how much less we had to type for the piping operator implementation. This is why we use the piping operator: it keeps things short and sweet and helps keep your Environment (recall this is in the top right panel of your RStudio session) as decluttered as possible by helping to prevent you from having to declare tons of intermediate variables (such as `sum_terms_1_and_2` and `sum_terms_2_and_3`).

A few remarks on the piping operator:

* Recall that an object can be more or less anything: a dataframe, a variable, a function... you name it.
* If the function specified immediately after the piping operator does not take at least one argument, using the piping operator with an object will result in an error.
* You can (and typically should) still use the piping operator with a function that takes multiple arguments. Just understand that you can only pipe one argument into the function.

You will explore some of these properties in the following exercises:

### Exercise: The Piping Operator #1

Earlier when processing the *Baseball Reference* data, I used the following lines of code:

```{}
batting_leaderboard_23 = remove_lg_avg_summary(batting_leaderboard_23)

batting_leaderboard_23 = remove_special_characters(batting_leaderboard_23, "[*#]")

batting_leaderboard_23 = identify_numeric_columns(batting_leaderboard_23)
```

Rewrite this code to use the piping operator so it is more compact. To keep our Environment clean, name your dataframe `bl_23_piping` instead of `bl_23.` This is to ensure the result of your notebook runs smoothly even if you answer here is incorrect.
```{r}
# YOUR SOLUTION HERE
```

### Exercise: The Piping Operator #2
Suppose I ask you to evaluate this expression: `3 + 5 / (4 + 3 *2 )`. Using the piping operator as much as possible, write code to print the answer:
```{r}
# YOUR SOLUTION HERE
```

Now that we are familiar with the piping operator, we can explore the core functions that make up the tidyverse.

## `SELECT`
**select** is a tidyverse function that lets you specify a subset of columns in a dataframe. Suppose you just want to see the number of doubles, triples, and homeruns for every player (row). *select* lets us do this in a nice straightforward manner.

```{r}
box_score_23 = batting_leaderboard_23 %>%
  select(Name, `2B`, `3B`, HR)
```

One remark here: note that any columns whose names start with a numeric character (e.g. 2, 3) must be wrapped in the \` character on each side. Be sure to use this syntax when referring to columns that start with a number as the first character in their name. No need to do it for columns where this isn't the case (such as e.g. HR).

### Exercise: `SELECT` #1
Locate the "Console", which in RStudio should be the bottom left of your current window (the "Console" tab is to the left of the "Terminal" and "Background Jobs" tabs... make sure you have clicked on "Console"). This is a space where you can run code that still creates variables in your environment but isn't part of the overall script or document you are working on. This is great for one-off commands to explore your data.

In the Console area, you should see what looks like a greater than sign. Click right next to it and type the following command: `colnames(batting_leaderboard_23)` (note that the \` on the front and back are just for fomatting in this current document, don't include them when putting this command in the console). Note that this shows you a *vector* (like a list) of every column in the dataframe `batting_leaderboard_23`. 

Take a subset of the columns of your choosing, and call *select* such that you produce a dataframe containing only these columns.

```{r}
# YOUR SOLUTION HERE
```

### Exercise: `SELECT` #2
* In the Environment tab at the top right, click on `box_score_23` after running the previous coding cell. Note that the labels at the top of the dataframe for each column will sort the dataframe by the respective column in different ways each time when you click on it. Use this to visually identify who led the league in each of doubles, triples, and homeruns in 2023.

```{}
Doubles leader: YOUR SOLUTION HERE
Tripes leader: YOUR SOLUTION HERE
Homeruns leader: YOUR SOLUTION HERE
```

# `MUTATE`
Being able to select columns is important but won't let you perform end-to-end data analysis on its own. Another function of the tidyverse that is very useful is **mutate**. Mutate lets you either adjust existing columns or (more commonly) create new columns, including by performing operations on other columns.

Note in the above example that the number of singles from each player isn't provided. It would be nice to have that in our dataframe. The mutate function lets us do this.

```{r}
bl23_with_singles = batting_leaderboard_23 %>%
  mutate(`1B` = H - (HR + `3B` + `2B`))
reactable(bl23_with_singles)
```

### Exercise: `MUTATE` #1

[**Batting average**](https://www.mlb.com/glossary/standard-stats/batting-average) is defined as total hits divided by total at bats. You can see that batting average is already a column in the dataframe (`BA`). Let's see if you can use mutate to calculate it properly for each player.

```{r}
bl23_calculating_BA = NULL
# REPLACE NULL WITH YOUR SOLUTION
```
### Exercise: `MUTATE` #2
[**Slugging percentage**](https://www.mlb.com/glossary/standard-stats/slugging-percentage) is defined as `(1B + 2 * 2B + 3 * 3B + 4 * HR) / AB`. Calculate slugging percentage using operations in a mutate function call. Make sure your estimation of slugging percentage matches the current `SLG` column.

```{r}
# YOUR SOLUTION HERE
```

### Exercise: `MUTATE` #3.
Read the definition for [**on-base percentage**](https://www.mlb.com/glossary/standard-stats/on-base-percentage). Update your implementation in the previous cell to calculate it using a call to *mutate*. After performing this computation, observe that `OPS` is defined as the sum of on-base percentage (`OBP`) and slugging (`SLG`). Write another mutate statement in the previous cell to compute `OPS` from scratch. Does your answer match the value in the initial `OPS` column of the dataset?

```{r}
# YOUR SOLUTION HERE
```

### Exercise: Reflection
No coding to do here. Just looking for your thoughts. At this point, you have analyzed the underlying calculation of batting average (`BA`), on-base percentage (`OBP`), slugging (`SLG`), and on-base-plus-slugging (`OPS`). In a paragraph or two, analyze the strengths and weaknesses of each metric. In relation to winning baseball games, which do you think provides the most complete picture of the offensive contributions of a player?

```{}
YOUR ANSWER HERE
```

## `GROUP BY`
Take closer look at our conditioned dataframe (`batting_leaderboard_23`). Do you notice anything weird about the rows?

If you look closely, you might notice that players who played on mulitiple teams during the 2023 season have multiple rows in the dataframe. For example, consider at Tommy Pham. Note that if you click on `batting_leaderboard_23` in the Environment tab at the top right, you will then see a little "Filter" button somewhat towards the top left of your screen. Click this "Filter" button and, in the search bar that appears under the `Name` column, type "Tommy Pham". Then click enter. Note that he has two rows: one corresponding to when he played for the New York Mets (NYM) and one for when he played with the Arizona Diamondbacks (ARI).

It would be nice here (and is often necessary) to have exactly one record per player season. Let's consolidate these two records for Tommy Pham (and any other players with comparable multi-team situations). Here's how we can do it:

```{r}
bl23_consolidated = batting_leaderboard_23 %>%
  group_by(Name) %>%
  mutate(across(where(is.double), ~mean(., na.rm = TRUE)),
         across(where(is.integer), ~sum(., na.rm = TRUE)))

```

There's a lot going on here. Let's dissect it:

* `group_by(Name)` suggests that we want to consider all players with a given `Name` value (e.g. Tommy Pham). Thinking back to our earlier example, this suggests that we want to consider all rows with `Name` "Tommy Pham" together.

* After specifying our grouping mechanisms with the `group_by`, we want to perform operations using `mutate` that aggregate row(s) that are applied to each *group* individually. However, think for a moment about the types of stats we have. Some are *volume*-based metrics (e.g. games played, plate appearance, hits, homeruns, etc.). Some are *rate*-based (e.g. batting average, on-base percentage, slugging percentage, on-base-plus-slugging, etc.). It makes sense to *sum* volume-based stats but *average* rate based stats. Note that Pham hit `.268` with the New York Mets and `.241` with the Arizona Diamondbacks in 2023, but saying that he hit `.268 + .241 = .509` would be both inaccurate and not sensible since batting average is a rate-based stat. On the other hand, Pham played `70` games with the Mets and `59` with the Diamondbacks and saying he played `70 + 59 = 129` games over the course of the 2023 season makes great sense since games played is a volume-based stat. We account for this distinction between volume-based stats and rate-based stats using the two cases--the `where(is.double)` case and the `where(is.integer)` case--separately. Specifically, a number here is a double if it is like a decimal (like a batting average, which is a decimal between 0.00 and 1.00) and an integer if it is a round number (like a games played value). You can see that we apply the `mean` function to the case involving `where(is.double)` (the rate-based metrics) and the `sum` function to the case involving `where(is.integer)` (the volume-based metrics).

* It's worth discussing the `across` function and its unique format in more detail. `across` is used within a `mutate` subject to the following format:
[dataframe] = [dataframe] %>%
  mutate(across([condition], ~[function([arguments])]))
In our example, we use our `batting_leaderboard_23` dataframe with the condition either being `where(is.integer)` or `where(is.double)` to extract a vector consisting of all columns with decimal-like data and all columns of round-number-like data, respectively. We then apply either the `mean` or `sum` function depending on the situation (rate-based metric vs. volume-based metric). Our [arguments] is either `sum(., na.rm = TRUE)` or `mean(., na.rm = TRUE)`. Here, the `.` refers to whatever vector has been selected via the `across` statement and [condition] (it is a column from the table). `na.rm = TRUE` means to remove any non-numeric or unknown values (often denoted as `NA`s) when performing the calculation and can be important to include to avoid encountering errors. Checkout the result:
```{r}
bl23_consolidated = batting_leaderboard_23 %>%
  group_by(Name) %>%
  mutate(across(where(is.double), ~mean(., na.rm = TRUE)),
            across(where(is.integer), ~sum(., na.rm = TRUE)))
reactable(bl23_consolidated, searchable = TRUE)
```

This is close to what we want, but look closely. Revisit our Tommy Pham example. You will note that he still appears in two rows: his Mets row and his Diamondbacks row are still present. But if you look closely, all of the stats in each of these rows are now equal. Why is this?

We see the stats are the same because we have mutated every stats column after grouping them together. So the stats you see in each Tommy Pham row are actually the mean (for rate-based stats) and sum (for volume-based stats) across all rows corresponding to Tommy Pham. However, we haven't performed any operations on the dataframe to reduce the number of rows. We can consolidate each of these rows (such as the multiple Tommy Pham) rows to one row per-player using distinct. Note a few things about our call to distinct:
* We specify the `Name` column within the distinct column to indicate that we want one row for every individual player `Name`.
* We specify `.keep_all = TRUE` to indicate that we want to keep all columns in the dataframe (that we want to keep all of the stats columns in addition to the `Name` for each player).

```{r}
bl23_consolidated = batting_leaderboard_23 %>%
  group_by(Name) %>%
  mutate(across(where(is.double), ~mean(., na.rm = TRUE)),
        across(where(is.integer), ~sum(., na.rm = TRUE))) %>%
  distinct(Name, .keep_all = TRUE)
reactable(bl23_consolidated, searchable = TRUE)
```

## `FILTER`
Sometimes it is useful to be able to parse our dataframe to only include rows satisfying certain criteria. We can accomplish this using the tidyverse [**filter**](https://dplyr.tidyverse.org/reference/filter.html) function.

Soon, we will calculate statistics exploring conceptions of an average MLB player. When doing such analyses, ensuring that the data you use to calculate these statistics is representative of the average MLB player is important. On one hand, we don't want to only include players who mostly play in the minor leagues and appear in the major leagues but have very few plate appearances in the major leagues. On the other hand, if we only permit players with an overly high amount of plate appearances, we will likely bias our dataset towards better players (assuming at a high level that better players get more at bats), in which case the picture we get from our calculated statistics will be distorted in the opposite direction. In this exercise, you will explore a variety of minimum plate apperance thresholds to try to balance the two aforementioned factors.

### Exercise: `Filtering` #2
This question requires no code but offers important context. What is the difference between an [**at bat (`AB`)**](https://www.mlb.com/glossary/standard-stats/at-bat) and a [**plate appearance (`PA`)**](https://www.mlb.com/glossary/standard-stats/plate-appearance)?

```{}
YOUR ANSWER HERE
```

### Exercise: `Filtering` #3
For simplicity, we will filter based on plate appearances (although at bats would be fine) just to ensure consistency among all students. Explore several minimum PA thresholds using filter statements. Take a look at the resulting dataframes. What thresholds seem potentaially reasonable? What thresholds seem unreasonable?

```{r}

```

# Exericse: `Filtering` #4
A useful command for visualizing distributions of variables in a dataset is the **summary** command. Try it here. Note that the summary function takes a vector (i.e. a column of a dataframe rather than the whole dataframe). We can access a specific column from the dataframe as a vector using the \$ notation: [dataframe]\$[column].
```{r}
summary(bl23_consolidated$PA)
```
Also note that we can easily extract the mean, median, standard deviation, max, min, sum etc. of a vector (and note that a column of a dataframe is a vector) using existing built-in functions from R.
```{r}
mean(bl23_consolidated$PA, na.rm = TRUE)
median(bl23_consolidated$PA, na.rm = TRUE)
sd(bl23_consolidated$PA, na.rm = TRUE)
max(bl23_consolidated$PA, na.rm = TRUE)
min(bl23_consolidated$PA, na.rm = TRUE)
sum(bl23_consolidated$PA, na.rm = TRUE)

```

## Pivoting and More Complex Mutates
Let's experiment a little bit more here. Suppose I want to explore the 

